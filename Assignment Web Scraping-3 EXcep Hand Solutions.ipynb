{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "#from any YouTube Video.\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "driver = webdriver.Chrome('path_to_chromedriver')\n",
    "video_url = 'https://www.youtube.com/watch?v=your_video_id'\n",
    "driver.get(video_url)\n",
    "scroll_pause_time = 2\n",
    "scrolls = 10\n",
    "for _ in range(scrolls):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    comments = driver.find_elements_by_xpath('//yt-formatted-string[@id=\"content-text\"]')\n",
    "    upvotes = driver.find_elements_by_xpath('//span[@id=\"vote-count-middle\"]')\n",
    "    times = driver.find_elements_by_xpath('//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "    extracted_data = []\n",
    "for comment, upvote, time in zip(comments, upvotes, times):\n",
    "  extracted_data.append({\n",
    "  'comment': comment.text,\n",
    "  'upvote': upvote.text,\n",
    "  'time': time.text\n",
    "  })\n",
    "driver.quit()\n",
    "for data in extracted_data:\n",
    "   print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ba99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_coordinates(city):\n",
    "  formatted_city = city.replace(\" \", \"+\")\n",
    "\n",
    "  url = f\"https://www.google.com/maps/search/{formatted_city}\"\n",
    "  response = requests.get(url)\n",
    "\n",
    "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "  coordinates_element = soup.find(\"meta\", itemprop=\"image\")\n",
    "\n",
    "  coordinates = coordinates_element[\"content\"].split(\";\")[1].strip().split(\",\")\n",
    "\n",
    "  return float(coordinates[0]), float(coordinates[1])\n",
    "\n",
    "city = input(\"Enter a city name: \")\n",
    "latitude, longitude = get_coordinates(city)\n",
    "print(f\"The coordinates of {city} are: Latitude: {latitude}, Longitude: {longitude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "#and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "#Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "#“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "#details is missing then replace it by “- “. Save your results in a dataframe and CSV. \n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r\"E:\\Aniket\\chromedriver_win32\\chromedriver.exe\")\n",
    "url4=\"https://www.flipkart.com/search?q=smartphone&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_Name=[]\n",
    "Colour=[]\n",
    "Storage_RAM_ROM=[]\n",
    "P_F_Camera=[]\n",
    "Display_size_Resolution=[]\n",
    "ProcessorAndCores=[]\n",
    "Battery=[]\n",
    "Price=[]\n",
    "Product_URL=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BName=driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "for i in BName:\n",
    "    if i.text is None :\n",
    "        Brand_Name.append(\"--\") \n",
    "    else:\n",
    "        Brand_Name.append(i.text)\n",
    "print(len(Brand_Name),Brand_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Storage_RAM_ROM \n",
    "ram=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[1]\")\n",
    "for i in ram:\n",
    "    if i.text is None :\n",
    "        Storage_RAM_ROM.append(\"--\") \n",
    "    else:\n",
    "        Storage_RAM_ROM.append(i.text)\n",
    "print(len(Storage_RAM_ROM),Storage_RAM_ROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a276c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the P_F_Camera \n",
    "PC=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[3]\")\n",
    "for i in PC:\n",
    "    if i.text is None :\n",
    "        P_F_Camera.append(\"--\") \n",
    "    else:\n",
    "        P_F_Camera.append(i.text)\n",
    "print(len(P_F_Camera),P_F_Camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b19a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Display_size_Resolution \n",
    "DS=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[2]\")\n",
    "for i in DS:\n",
    "    if i.text is None :\n",
    "        Display_size_Resolution.append(\"--\") \n",
    "    else:\n",
    "        Display_size_Resolution.append(i.text)\n",
    "print(len(Display_size_Resolution),Display_size_Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the ProcessorAndCores \n",
    "P=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[5]\")\n",
    "for i in P:\n",
    "    if i.text is None :\n",
    "        ProcessorAndCores.append(\"--\") \n",
    "    else:\n",
    "        ProcessorAndCores.append(i.text)\n",
    "print(len(ProcessorAndCores),ProcessorAndCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Battery \n",
    "B=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[4]\")\n",
    "for i in B:\n",
    "    if i.text is None :\n",
    "        Battery.append(\"--\") \n",
    "    else:\n",
    "        Battery.append(i.text)\n",
    "print(len(Battery),Battery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Price \n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3 _1_WHN1']\")\n",
    "for i in price:\n",
    "    if i.text is None :\n",
    "        Price.append(\"--\") \n",
    "    else:\n",
    "        Price.append(i.text)\n",
    "print(len(Price),Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd774e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FlipKart=pd.DataFrame([])\n",
    "FlipKart['Brand_Name']=Brand_Name\n",
    "FlipKart['Storage_RAM_ROM']=Storage_RAM_ROM\n",
    "FlipKart['Amount P_F_Camera']=P_F_Camera\n",
    "FlipKart['Display_size_Resolution']=Display_size_Resolution\n",
    "FlipKart['ProcessorAndCores']=ProcessorAndCores\n",
    "FlipKart['Battery']=Battery\n",
    "FlipKart['Price']=Price\n",
    "\n",
    "FlipKart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "#images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. \n",
    "\n",
    "driver.get('https://images.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4483311",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    \n",
    "search_bar.send_keys(\"fruits\")      \n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')    \n",
    "search_button.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start scrolling to generate more images on the page...\")\n",
    "# 500 time we scroll down by 10000 in order to generate more images on the website\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    " images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebe4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd574ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"H:/Flip ROBO/banana/img\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13448b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user.\n",
    "#For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputU = input('please enter product here--->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(inputU)       # Inputing keyword to search \n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')    # Finding the xpath of search button\n",
    "search_button.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31479719",
   "metadata": {},
   "outputs": [],
   "source": [
    "productName=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427771ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PName=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in PName:\n",
    "    if i.text is None :\n",
    "        productName.append(\"--\") \n",
    "    else:\n",
    "        productName.append(i.text)\n",
    "print(len(productName),productName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f752d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. In the above question, now scrape the following details of each product listed in first 3 pages\n",
    "#of your search results and save it in a dataframe and csv. In case if any product vertical has\n",
    "#less than 3 pages in search results then scrape all the products available under that product\n",
    "#vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of\n",
    "#Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\"\n",
    "#and “Product URL”. In case, if any of the details are missing for any of the product then\n",
    "#replace it by “-“.\n",
    "\n",
    "start_page = 0\n",
    "end_page = 3\n",
    "urls = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    try:\n",
    "        page_urls = driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "        \n",
    "       \n",
    "        for url in page_urls:\n",
    "            url = url.get_attribute('href')     \n",
    "            if url[0:4]=='http':                \n",
    "                urls.append(url)                \n",
    "        print(\"Product urls of page {} has been scraped.\".format(page+1))\n",
    "        \n",
    "        # Moving to next page\n",
    "        nxt_button = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')      \n",
    "        if nxt_button.text == 'Next→':                                            \n",
    "            nxt_button.click()                                                    \n",
    "            time.sleep(5)                                                        \n",
    "           \n",
    "        elif driver.find_element_by_xpath('//li[@class=\"a-disabled a-last\"]/a').text == 'Next→':    \n",
    "            print(\"No new pages exist. Breaking the loop\") \n",
    "            break\n",
    "            \n",
    "    except StaleElementReferenceException as e:              \n",
    "        print(\"Stale Exception\")\n",
    "        next_page = nxt_button.get_attribute('href')       \n",
    "        driver.get(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ff50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = {}\n",
    "prod_dict['Brand']=[]\n",
    "prod_dict['Name']=[]\n",
    "prod_dict['Rating']=[]\n",
    "prod_dict['No. of ratings']=[]\n",
    "prod_dict['Price']=[]\n",
    "prod_dict['Return/Exchange']=[]\n",
    "prod_dict['Expected Delivery']=[] \n",
    "prod_dict['Availability']=[]\n",
    "prod_dict['Other Details']=[]\n",
    "prod_dict['URL']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "or url in urls[:4]:\n",
    "    driver.get(url)                                                       \n",
    "    print(\"Scraping URL = \", url)\n",
    "    #time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')      \n",
    "        prod_dict['Brand'].append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Brand'].append('-')\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//h1[@id=\"title\"]/span')      \n",
    "        prod_dict['Name'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Name'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('//span[@id=\"acrPopover\"]')  \n",
    "        prod_dict['Rating'].append(rating.get_attribute(\"title\"))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Rating'].append('-')\n",
    "    \n",
    "    try:\n",
    "        n_rating = driver.find_element_by_xpath('//a[@id=\"acrCustomerReviewLink\"]/span')      \n",
    "        prod_dict['No. of ratings'].append(n_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['No. of ratings'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//span[@id=\"priceblock_ourprice\"]')           \n",
    "        prod_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Price'].append('-')\n",
    "    try:                                                                                      \n",
    "        ret = driver.find_element_by_xpath('//div[@data-name=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        prod_dict['Return/Exchange'].append(ret.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Return/Exchange'].append('-')\n",
    "    try:\n",
    "        delivry = driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')         \n",
    "        prod_dict['Expected Delivery'].append(delivry.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Expected Delivery'].append('-')\n",
    "    \n",
    "    try:\n",
    "        avl = driver.find_element_by_xpath('//div[@id=\"availability\"]/span')                \n",
    "        prod_dict['Availability'].append(avl.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Availability'].append('-')\n",
    "    \n",
    "    try:                                                                                   \n",
    "        dtls = driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        prod_dict['Other Details'].append('  ||  '.join(dtls.text.split('\\n')))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Other Details'].append('-')\n",
    "    \n",
    "    prod_dict['URL'].append(url)                                                           \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86686593",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame.from_dict(prod_dict)\n",
    "prod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.to_csv('Amazon_{}.csv'.format(inputU))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
